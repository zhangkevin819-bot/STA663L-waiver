# Main configuration file for Hydra
# Override with: python main.py data.batch_size=128 model.hidden_dim=1024

defaults:
  - _self_

# Data configuration
data:
  raw_path: data/raw
  processed_path: data/processed
  batch_size: 256
  num_workers: 4
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  seed: 42
  use_polars: true
  cache_enabled: true

# Model configuration
model:
  name: transformer_classifier
  hidden_dim: 512
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  activation: gelu
  use_flash_attention: true
  pretrained: null

# Training configuration
training:
  epochs: 100
  learning_rate: 3e-4
  weight_decay: 1e-5
  optimizer: adamw
  scheduler: cosine_with_warmup
  warmup_steps: 1000
  gradient_clip: 1.0
  mixed_precision: true
  compile_model: true
  early_stopping_patience: 10
  gradient_accumulation_steps: 1

# Inference configuration
inference:
  device: cuda
  batch_size: 64
  num_workers: 2
  enable_onnx: false
  quantization: null
  optimization_level: 2

# Logging configuration
logging:
  level: INFO
  log_dir: logs
  mlflow_uri: http://localhost:5000
  experiment_name: advanced_ml_pipeline
  track_gradients: true
  log_every_n_steps: 50

# API configuration
api:
  host: 0.0.0.0
  port: 8000
  workers: 4
  reload: false
  log_level: info
  cors_origins:
    - "*"
  rate_limit: 100
  timeout: 30.0

# Global settings
project_root: ${hydra:runtime.cwd}
environment: development
debug: false

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
